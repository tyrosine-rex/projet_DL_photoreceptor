{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FASTER R-CNN\n",
    "\n",
    "paper:https://arxiv.org/abs/1506.01497\n",
    "\n",
    "for training:\n",
    "- input Channel(C) x Height(H) x Width(W) images as torch.Tensor[C, H, W] object\n",
    "- N boxes coordinates : xmin, ymin, xmax, ymax (in this order) as torch.FloatTensor[N, 4]\n",
    "- labels for each N boxes: \"photoreceptor\" as torch.Int64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/thomas/.mambaforge/envs/ommatidia/lib/python3.9/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "from torchvision import datasets \n",
    "from torchvision import transforms \n",
    "from torchvision.io import read_image\n",
    "\n",
    "from pathlib import Path \n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "FAKE_DATA_DIR=\"../../res/fake_vignette/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "definition du modele"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
    "from torchvision.models.detection import fasterrcnn_resnet50_fpn, FasterRCNN_ResNet50_FPN_Weights\n",
    "\n",
    "\n",
    "def make_model():\n",
    "    model = fasterrcnn_resnet50_fpn(weights=FasterRCNN_ResNet50_FPN_Weights.DEFAULT)\n",
    "\n",
    "    num_classes = 2 \n",
    "    in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
    "\n",
    "    model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Definition de notre classe dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FakeImgDataset(Dataset): \n",
    "    def __init__(self, root):\n",
    "        self.root = Path(root)\n",
    "        self.path_imgs = list(self.root.glob(\"imgs/*.png\"))\n",
    "        self.df = pd.read_csv(self.root/\"annotations.csv\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.path_imgs)\n",
    "\n",
    "    def __getitem__(self, idx): \n",
    "        #lazy eval\n",
    "\n",
    "        # get image to Tensor\n",
    "        image = read_image(self.path_imgs[idx].as_posix(), torchvision.io.ImageReadMode.GRAY)/255\n",
    "        \n",
    "        #get boxes\n",
    "        image_name = self.path_imgs[idx].name\n",
    "        np_boxes = self.df[self.df.filename == image_name].iloc[:, 2:].values\n",
    "        boxes = torch.FloatTensor(np_boxes)\n",
    "\n",
    "        #get labels, just 1, we have only 1 class\n",
    "        labels = torch.ones(boxes.shape[0], dtype=torch.int64)\n",
    "\n",
    "        targets = {\n",
    "            \"boxes\" : boxes,\n",
    "            \"labels\" : labels\n",
    "        }\n",
    "\n",
    "        return image, targets\n",
    "\n",
    "def my_collate_fun(batch): # wrap the outpur of __getitem__() for DataLoader\n",
    "    im_lst, tgt_lst = [], []\n",
    "    for im, tgt in batch:\n",
    "        im_lst.append(im)\n",
    "        tgt_lst.append(tgt)\n",
    "    return im_lst, tgt_lst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On instencie note classe dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "fake_train_dataset = FakeImgDataset(FAKE_DATA_DIR)\n",
    "\n",
    "fake_train_loader = torch.utils.data.DataLoader(    \n",
    "    fake_train_dataset, batch_size=10, shuffle=True, collate_fn=my_collate_fun, drop_last=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On instancie notre model et on définit l'optimiseur à utiliser (classique avec un SDG -> stochastic descent gradient)\n",
    "learning rate à 0.005 au début puis diminura toutes les 3 epochs "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = make_model()\n",
    "params = [p for p in model.parameters() if p.requires_grad]\n",
    "optimizer = torch.optim.SGD(params, lr=0.005, momentum=0.9)#, weight_decay=0.0005)\n",
    "lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=3, gamma=0.1)\n",
    "num_epochs = 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\titer:0, loss:1.263409972190857\n",
      "\titer:1, loss:nan\n",
      "\titer:2, loss:nan\n",
      "\titer:3, loss:nan\n",
      "\titer:4, loss:nan\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [22], line 13\u001b[0m\n\u001b[1;32m     10\u001b[0m loss_value \u001b[39m=\u001b[39m losses\u001b[39m.\u001b[39mitem()\n\u001b[1;32m     12\u001b[0m optimizer\u001b[39m.\u001b[39mzero_grad()\n\u001b[0;32m---> 13\u001b[0m losses\u001b[39m.\u001b[39;49mbackward()\n\u001b[1;32m     14\u001b[0m optimizer\u001b[39m.\u001b[39mstep()\n\u001b[1;32m     16\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m\\t\u001b[39;00m\u001b[39miter:\u001b[39m\u001b[39m{\u001b[39;00mitr\u001b[39m}\u001b[39;00m\u001b[39m, loss:\u001b[39m\u001b[39m{\u001b[39;00mloss_value\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[0;32m~/.mambaforge/envs/ommatidia/lib/python3.9/site-packages/torch/_tensor.py:487\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    477\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function_unary(\u001b[39mself\u001b[39m):\n\u001b[1;32m    478\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    479\u001b[0m         Tensor\u001b[39m.\u001b[39mbackward,\n\u001b[1;32m    480\u001b[0m         (\u001b[39mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    485\u001b[0m         inputs\u001b[39m=\u001b[39minputs,\n\u001b[1;32m    486\u001b[0m     )\n\u001b[0;32m--> 487\u001b[0m torch\u001b[39m.\u001b[39;49mautograd\u001b[39m.\u001b[39;49mbackward(\n\u001b[1;32m    488\u001b[0m     \u001b[39mself\u001b[39;49m, gradient, retain_graph, create_graph, inputs\u001b[39m=\u001b[39;49minputs\n\u001b[1;32m    489\u001b[0m )\n",
      "File \u001b[0;32m~/.mambaforge/envs/ommatidia/lib/python3.9/site-packages/torch/autograd/__init__.py:197\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    192\u001b[0m     retain_graph \u001b[39m=\u001b[39m create_graph\n\u001b[1;32m    194\u001b[0m \u001b[39m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[1;32m    195\u001b[0m \u001b[39m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    196\u001b[0m \u001b[39m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 197\u001b[0m Variable\u001b[39m.\u001b[39;49m_execution_engine\u001b[39m.\u001b[39;49mrun_backward(  \u001b[39m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    198\u001b[0m     tensors, grad_tensors_, retain_graph, create_graph, inputs,\n\u001b[1;32m    199\u001b[0m     allow_unreachable\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, accumulate_grad\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for epoch in range(num_epochs):\n",
    "\n",
    "    for itr, (images, targets) in enumerate(fake_train_loader):\n",
    "        \n",
    "        images = [im.to(device) for im in images]\n",
    "        targets = [{k: v.to(device) for k, v in tgt.items()} for tgt in targets]\n",
    "\n",
    "        loss_dict = model(images, targets)\n",
    "        losses = sum(loss for loss in loss_dict.values())\n",
    "        loss_value = losses.item()\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        losses.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        print(f\"\\titer:{itr}, loss:{loss_value}\")\n",
    "\n",
    "        lr_scheduler.step()\n",
    "    \n",
    "    print(f\"epoch:{epoch}/{num_epochs}, loss:{loss_value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# criterion = nn.CrossEntropyLoss()\n",
    "# model1 = make_model()\n",
    "# params = [p for p in model1.parameters() if p.requires_grad]\n",
    "# optimizer = optim.SGD(params, lr=0.001, momentum=0.9)\n",
    "\n",
    "# num_epochs = 10\n",
    "\n",
    "# for epoch in range(num_epochs):\n",
    "#     # train for one epoch, printing every 10 iterations\n",
    "#     train_one_epoch(model, optimizer, data_loader, device, epoch, print_freq=10)\n",
    "#     # update the learning rate\n",
    "#     lr_scheduler.step()\n",
    "#     # evaluate on the test dataset\n",
    "#     evaluate(model, data_loader_test, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(train_data_loader, model):\n",
    "    print('Training')\n",
    "    global train_itr\n",
    "    global train_loss_list\n",
    "    \n",
    "     # initialize tqdm progress bar\n",
    "    prog_bar = tqdm(train_data_loader, total=len(train_data_loader))\n",
    "    \n",
    "    for i, data in enumerate(prog_bar):\n",
    "        optimizer.zero_grad()\n",
    "        images, targets = data\n",
    "        \n",
    "        images = list(image.to(DEVICE) for image in images)\n",
    "        targets = [{k: v.to(DEVICE) for k, v in t.items()} for t in targets]\n",
    "        loss_dict = model(images, targets)\n",
    "        losses = sum(loss for loss in loss_dict.values())\n",
    "        loss_value = losses.item()\n",
    "        train_loss_list.append(loss_value)\n",
    "        train_loss_hist.send(loss_value)\n",
    "        losses.backward()\n",
    "        optimizer.step()\n",
    "        train_itr += 1\n",
    "    \n",
    "        # update the loss value beside the progress bar for each iteration\n",
    "        prog_bar.set_description(desc=f\"Loss: {loss_value:.4f}\")\n",
    "    return train_loss_list"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.15 ('ommatidia')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "a0e42b51d21f453e230271c96ebadabb38c1a4156efcce4ceb052509a2508e50"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
